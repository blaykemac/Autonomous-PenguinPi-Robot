Net Architecture:
Resnet18Skip(
  (res18_backbone): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  )
  (conv2_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv3_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv4_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv5_x): Sequential(
    (0): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (top_conv): Sequential(
    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv1): Sequential(
    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv2): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (lateral_conv3): Sequential(
    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
    (1): ReLU()
  )
  (segmentation_conv): Sequential(
    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))
  )
  (criterion): CrossEntropyLoss()
)
Loss Function: CrossEntropyLoss

===========================================================
==================== Hyper-parameters =====================
n_classes: 3
lr: 0.001
epochs: 40
batch_size: 64
weight_decay: 0.0001
scheduler_step: 5
scheduler_gamma: 0.5
model_dir: model_second
load_best: 0
log_freq: 20
dataset_dir: /home/blayke/catkin_ws/src/data_collector/dataset/
===========================================================
============= Epoch 0 | 2021-10-10 07:35:28 ===============
=> Current Lr: 0.001
[0/375]: 1.7950
[20/375]: 0.0942
[40/375]: 0.0704
[60/375]: 0.0765
[80/375]: 0.0619
[100/375]: 0.0496
[120/375]: 0.0543
[140/375]: 0.0521
[160/375]: 0.0366
[180/375]: 0.0443
[200/375]: 0.0486
[220/375]: 0.0372
[240/375]: 0.0390
[260/375]: 0.0489
[280/375]: 0.0293
[300/375]: 0.0470
[320/375]: 0.0350
[340/375]: 0.0395
[360/375]: 0.0352
=> Training Loss: 0.0597, Evaluation Loss 0.0342

============= Epoch 1 | 2021-10-10 07:37:28 ===============
=> Current Lr: 0.001
[0/375]: 0.0262
[20/375]: 0.0232
[40/375]: 0.0316
[60/375]: 0.0512
[80/375]: 0.0424
[100/375]: 0.0283
[120/375]: 0.0314
[140/375]: 0.0308
[160/375]: 0.0321
[180/375]: 0.0287
[200/375]: 0.0250
[220/375]: 0.0234
[240/375]: 0.0273
[260/375]: 0.0220
[280/375]: 0.0192
[300/375]: 0.0320
[320/375]: 0.0327
[340/375]: 0.0292
[360/375]: 0.0197
=> Training Loss: 0.0304, Evaluation Loss 0.0327

============= Epoch 2 | 2021-10-10 07:39:28 ===============
=> Current Lr: 0.001
[0/375]: 0.0216
[20/375]: 0.0252
[40/375]: 0.0217
[60/375]: 0.0185
[80/375]: 0.0369
[100/375]: 0.0234
[120/375]: 0.0458
[140/375]: 0.0248
[160/375]: 0.0240
[180/375]: 0.0234
[200/375]: 0.0177
[220/375]: 0.0208
[240/375]: 0.0277
[260/375]: 0.0215
[280/375]: 0.0262
[300/375]: 0.0174
[320/375]: 0.0220
[340/375]: 0.0245
[360/375]: 0.0185
=> Training Loss: 0.0255, Evaluation Loss 0.0225

============= Epoch 3 | 2021-10-10 07:41:29 ===============
=> Current Lr: 0.001
[0/375]: 0.0120
[20/375]: 0.0235
[40/375]: 0.0202
[60/375]: 0.0224
[80/375]: 0.0179
[100/375]: 0.0193
[120/375]: 0.0154
[140/375]: 0.0203
[160/375]: 0.0172
[180/375]: 0.0217
[200/375]: 0.0131
[220/375]: 0.0241
[240/375]: 0.0132
[260/375]: 0.0390
[280/375]: 0.0438
[300/375]: 0.0223
[320/375]: 0.0177
[340/375]: 0.0198
[360/375]: 0.0220
=> Training Loss: 0.0224, Evaluation Loss 0.0246

============= Epoch 4 | 2021-10-10 07:43:27 ===============
=> Current Lr: 0.001
[0/375]: 0.0210
[20/375]: 0.0166
[40/375]: 0.0215
[60/375]: 0.0167
[80/375]: 0.0164
[100/375]: 0.0217
[120/375]: 0.0420
[140/375]: 0.0178
[160/375]: 0.0139
[180/375]: 0.0228
[200/375]: 0.0385
[220/375]: 0.0330
[240/375]: 0.0130
[260/375]: 0.0230
[280/375]: 0.0208
[300/375]: 0.0162
[320/375]: 0.0155
[340/375]: 0.0206
[360/375]: 0.0302
=> Training Loss: 0.0208, Evaluation Loss 0.0216

============= Epoch 5 | 2021-10-10 07:45:24 ===============
=> Current Lr: 0.0005
[0/375]: 0.0193
[20/375]: 0.0186
[40/375]: 0.0120
[60/375]: 0.0137
[80/375]: 0.0131
[100/375]: 0.0146
[120/375]: 0.0237
[140/375]: 0.0122
[160/375]: 0.0140
[180/375]: 0.0104
[200/375]: 0.0126
[220/375]: 0.0170
[240/375]: 0.0112
[260/375]: 0.0100
[280/375]: 0.0127
[300/375]: 0.0127
[320/375]: 0.0205
[340/375]: 0.0101
[360/375]: 0.0121
=> Training Loss: 0.0150, Evaluation Loss 0.0146

============= Epoch 6 | 2021-10-10 07:47:22 ===============
=> Current Lr: 0.0005
[0/375]: 0.0109
[20/375]: 0.0092
[40/375]: 0.0162
[60/375]: 0.0131
[80/375]: 0.0110
[100/375]: 0.0137
[120/375]: 0.0094
[140/375]: 0.0115
[160/375]: 0.0150
[180/375]: 0.0139
[200/375]: 0.0107
[220/375]: 0.0119
[240/375]: 0.0324
[260/375]: 0.0842
[280/375]: 0.0158
[300/375]: 0.0203
[320/375]: 0.0140
[340/375]: 0.0187
[360/375]: 0.0146
=> Training Loss: 0.0151, Evaluation Loss 0.0191

============= Epoch 7 | 2021-10-10 07:49:20 ===============
=> Current Lr: 0.0005
[0/375]: 0.0256
[20/375]: 0.0165
[40/375]: 0.0151
[60/375]: 0.0171
[80/375]: 0.0138
[100/375]: 0.0195
[120/375]: 0.0146
[140/375]: 0.0147
[160/375]: 0.0142
[180/375]: 0.0128
[200/375]: 0.0166
[220/375]: 0.0114
[240/375]: 0.0141
[260/375]: 0.0144
[280/375]: 0.0164
[300/375]: 0.0090
[320/375]: 0.0133
[340/375]: 0.0117
[360/375]: 0.0138
=> Training Loss: 0.0145, Evaluation Loss 0.0186

============= Epoch 8 | 2021-10-10 07:51:18 ===============
=> Current Lr: 0.0005
[0/375]: 0.0178
[20/375]: 0.0132
[40/375]: 0.0115
[60/375]: 0.0109
[80/375]: 0.0117
[100/375]: 0.0086
[120/375]: 0.0190
[140/375]: 0.0193
[160/375]: 0.0168
[180/375]: 0.0108
[200/375]: 0.0127
[220/375]: 0.0127
[240/375]: 0.0166
[260/375]: 0.0212
[280/375]: 0.0122
[300/375]: 0.0132
[320/375]: 0.0106
[340/375]: 0.0148
[360/375]: 0.0131
=> Training Loss: 0.0141, Evaluation Loss 0.0169

============= Epoch 9 | 2021-10-10 07:53:15 ===============
=> Current Lr: 0.0005
[0/375]: 0.0221
[20/375]: 0.0272
[40/375]: 0.0113
[60/375]: 0.0109
[80/375]: 0.0094
[100/375]: 0.0114
[120/375]: 0.0105
[140/375]: 0.0100
[160/375]: 0.0212
[180/375]: 0.0099
[200/375]: 0.0116
[220/375]: 0.0247
[240/375]: 0.0195
[260/375]: 0.0118
[280/375]: 0.0139
[300/375]: 0.0123
[320/375]: 0.0115
[340/375]: 0.0132
[360/375]: 0.0150
=> Training Loss: 0.0135, Evaluation Loss 0.0122

============= Epoch 10 | 2021-10-10 07:55:14 ==============
=> Current Lr: 0.00025
[0/375]: 0.0116
[20/375]: 0.0150
[40/375]: 0.0081
[60/375]: 0.0110
[80/375]: 0.0129
[100/375]: 0.0155
[120/375]: 0.0142
[140/375]: 0.0104
[160/375]: 0.0091
[180/375]: 0.0115
[200/375]: 0.0070
[220/375]: 0.0119
[240/375]: 0.0094
[260/375]: 0.0103
[280/375]: 0.0064
[300/375]: 0.0130
[320/375]: 0.0085
[340/375]: 0.0182
[360/375]: 0.0077
=> Training Loss: 0.0107, Evaluation Loss 0.0202

============= Epoch 11 | 2021-10-10 07:57:12 ==============
=> Current Lr: 0.00025
[0/375]: 0.0183
[20/375]: 0.0103
[40/375]: 0.0125
[60/375]: 0.0119
[80/375]: 0.0123
[100/375]: 0.0094
[120/375]: 0.0101
[140/375]: 0.0096
[160/375]: 0.0094
[180/375]: 0.0058
[200/375]: 0.0106
[220/375]: 0.0102
[240/375]: 0.0120
[260/375]: 0.0094
[280/375]: 0.0100
[300/375]: 0.0093
[320/375]: 0.0078
[340/375]: 0.0076
[360/375]: 0.0089
=> Training Loss: 0.0110, Evaluation Loss 0.0104

============= Epoch 12 | 2021-10-10 07:59:10 ==============
=> Current Lr: 0.00025
[0/375]: 0.0108
[20/375]: 0.0096
[40/375]: 0.0096
[60/375]: 0.0116
[80/375]: 0.0072
[100/375]: 0.0133
[120/375]: 0.0082
[140/375]: 0.0076
[160/375]: 0.0110
[180/375]: 0.0100
[200/375]: 0.0081
[220/375]: 0.0095
[240/375]: 0.0068
[260/375]: 0.0096
[280/375]: 0.0103
[300/375]: 0.0114
[320/375]: 0.0099
[340/375]: 0.0111
[360/375]: 0.0104
=> Training Loss: 0.0104, Evaluation Loss 0.0106

============= Epoch 13 | 2021-10-10 08:01:10 ==============
=> Current Lr: 0.00025
[0/375]: 0.0125
[20/375]: 0.0089
[40/375]: 0.0119
[60/375]: 0.0067
[80/375]: 0.0095
[100/375]: 0.0085
[120/375]: 0.0119
[140/375]: 0.0083
[160/375]: 0.0115
[180/375]: 0.0092
[200/375]: 0.0097
[220/375]: 0.0088
[240/375]: 0.0103
[260/375]: 0.0076
[280/375]: 0.0097
[300/375]: 0.0113
[320/375]: 0.0089
[340/375]: 0.0091
[360/375]: 0.0071
=> Training Loss: 0.0099, Evaluation Loss 0.0101

============= Epoch 14 | 2021-10-10 08:03:13 ==============
=> Current Lr: 0.00025
[0/375]: 0.0095
[20/375]: 0.0112
[40/375]: 0.0100
[60/375]: 0.0087
[80/375]: 0.0067
[100/375]: 0.0076
[120/375]: 0.0092
[140/375]: 0.0105
[160/375]: 0.0093
[180/375]: 0.0102
[200/375]: 0.0152
[220/375]: 0.0126
[240/375]: 0.0110
[260/375]: 0.0097
[280/375]: 0.0119
[300/375]: 0.0091
[320/375]: 0.0147
[340/375]: 0.0097
[360/375]: 0.0094
=> Training Loss: 0.0103, Evaluation Loss 0.0103

============= Epoch 15 | 2021-10-10 08:05:16 ==============
=> Current Lr: 0.000125
[0/375]: 0.0090
[20/375]: 0.0096
[40/375]: 0.0074
[60/375]: 0.0058
[80/375]: 0.0075
[100/375]: 0.0073
[120/375]: 0.0085
[140/375]: 0.0076
[160/375]: 0.0105
[180/375]: 0.0084
[200/375]: 0.0076
[220/375]: 0.0098
[240/375]: 0.0085
[260/375]: 0.0079
[280/375]: 0.0081
[300/375]: 0.0104
[320/375]: 0.0068
[340/375]: 0.0097
[360/375]: 0.0077
=> Training Loss: 0.0093, Evaluation Loss 0.0089

============= Epoch 16 | 2021-10-10 08:07:18 ==============
=> Current Lr: 0.000125
[0/375]: 0.0076
[20/375]: 0.0096
[40/375]: 0.0090
[60/375]: 0.0101
[80/375]: 0.0092
[100/375]: 0.0101
[120/375]: 0.0083
[140/375]: 0.0111
[160/375]: 0.0074
[180/375]: 0.0136
[200/375]: 0.0096
[220/375]: 0.0082
[240/375]: 0.0105
[260/375]: 0.0073
[280/375]: 0.0057
[300/375]: 0.0069
[320/375]: 0.0092
[340/375]: 0.0068
[360/375]: 0.0082
=> Training Loss: 0.0086, Evaluation Loss 0.0084

============= Epoch 17 | 2021-10-10 08:09:17 ==============
=> Current Lr: 0.000125
[0/375]: 0.0089
[20/375]: 0.0068
[40/375]: 0.0068
[60/375]: 0.0067
[80/375]: 0.0062
[100/375]: 0.0060
[120/375]: 0.0057
[140/375]: 0.0074
[160/375]: 0.0083
[180/375]: 0.0067
[200/375]: 0.0073
[220/375]: 0.0075
[240/375]: 0.0080
[260/375]: 0.0097
[280/375]: 0.0100
[300/375]: 0.0054
[320/375]: 0.0095
[340/375]: 0.0079
[360/375]: 0.0054
=> Training Loss: 0.0083, Evaluation Loss 0.0081

============= Epoch 18 | 2021-10-10 08:11:16 ==============
=> Current Lr: 0.000125
[0/375]: 0.0083
[20/375]: 0.0098
[40/375]: 0.0139
[60/375]: 0.0074
[80/375]: 0.0091
[100/375]: 0.0088
[120/375]: 0.0099
[140/375]: 0.0068
[160/375]: 0.0071
[180/375]: 0.0068
[200/375]: 0.0100
[220/375]: 0.0080
[240/375]: 0.0083
[260/375]: 0.0065
[280/375]: 0.0068
[300/375]: 0.0098
[320/375]: 0.0092
[340/375]: 0.0092
[360/375]: 0.0122
=> Training Loss: 0.0087, Evaluation Loss 0.0085

============= Epoch 19 | 2021-10-10 08:13:16 ==============
=> Current Lr: 0.000125
[0/375]: 0.0112
[20/375]: 0.0068
[40/375]: 0.0075
[60/375]: 0.0077
[80/375]: 0.0088
[100/375]: 0.0056
[120/375]: 0.0078
[140/375]: 0.0096
[160/375]: 0.0082
[180/375]: 0.0107
[200/375]: 0.0150
[220/375]: 0.0121
[240/375]: 0.0072
[260/375]: 0.0102
[280/375]: 0.0089
[300/375]: 0.0071
[320/375]: 0.0081
[340/375]: 0.0109
[360/375]: 0.0101
=> Training Loss: 0.0085, Evaluation Loss 0.0083

============= Epoch 20 | 2021-10-10 08:15:15 ==============
=> Current Lr: 6.25e-05
[0/375]: 0.0111
[20/375]: 0.0066
[40/375]: 0.0120
[60/375]: 0.0067
[80/375]: 0.0081
[100/375]: 0.0057
[120/375]: 0.0092
[140/375]: 0.0070
[160/375]: 0.0104
[180/375]: 0.0057
[200/375]: 0.0135
[220/375]: 0.0083
[240/375]: 0.0062
[260/375]: 0.0078
[280/375]: 0.0069
[300/375]: 0.0064
[320/375]: 0.0060
[340/375]: 0.0444
[360/375]: 0.0063
=> Training Loss: 0.0078, Evaluation Loss 0.0076

============= Epoch 21 | 2021-10-10 08:17:14 ==============
=> Current Lr: 6.25e-05
[0/375]: 0.0081
[20/375]: 0.0061
[40/375]: 0.0053
[60/375]: 0.0078
[80/375]: 0.0071
[100/375]: 0.0077
[120/375]: 0.0127
[140/375]: 0.0062
[160/375]: 0.0075
[180/375]: 0.0067
[200/375]: 0.0079
[220/375]: 0.0077
[240/375]: 0.0075
[260/375]: 0.0074
[280/375]: 0.0063
[300/375]: 0.0054
[320/375]: 0.0083
[340/375]: 0.0069
[360/375]: 0.0085
=> Training Loss: 0.0076, Evaluation Loss 0.0073

============= Epoch 22 | 2021-10-10 08:19:13 ==============
=> Current Lr: 6.25e-05
[0/375]: 0.0094
[20/375]: 0.0069
[40/375]: 0.0061
[60/375]: 0.0075
[80/375]: 0.0077
[100/375]: 0.0081
[120/375]: 0.0069
[140/375]: 0.0066
[160/375]: 0.0055
[180/375]: 0.0077
[200/375]: 0.0094
[220/375]: 0.0043
[240/375]: 0.0104
[260/375]: 0.0051
[280/375]: 0.0059
[300/375]: 0.0097
[320/375]: 0.0060
[340/375]: 0.0094
[360/375]: 0.0092
=> Training Loss: 0.0077, Evaluation Loss 0.0076

============= Epoch 23 | 2021-10-10 08:21:12 ==============
=> Current Lr: 6.25e-05
[0/375]: 0.0051
[20/375]: 0.0057
[40/375]: 0.0074
[60/375]: 0.0104
[80/375]: 0.0073
[100/375]: 0.0162
[120/375]: 0.0091
[140/375]: 0.0084
[160/375]: 0.0077
[180/375]: 0.0057
[200/375]: 0.0055
[220/375]: 0.0074
[240/375]: 0.0061
[260/375]: 0.0073
[280/375]: 0.0068
[300/375]: 0.0064
[320/375]: 0.0083
[340/375]: 0.0071
[360/375]: 0.0063
=> Training Loss: 0.0079, Evaluation Loss 0.0075

============= Epoch 24 | 2021-10-10 08:23:11 ==============
=> Current Lr: 6.25e-05
[0/375]: 0.0095
[20/375]: 0.0077
[40/375]: 0.0098
[60/375]: 0.0072
[80/375]: 0.0067
[100/375]: 0.0057
[120/375]: 0.0064
[140/375]: 0.0124
[160/375]: 0.0072
[180/375]: 0.0045
[200/375]: 0.0070
[220/375]: 0.0084
[240/375]: 0.0059
[260/375]: 0.0097
[280/375]: 0.0081
[300/375]: 0.0039
[320/375]: 0.0077
[340/375]: 0.0084
[360/375]: 0.0098
=> Training Loss: 0.0073, Evaluation Loss 0.0074

============= Epoch 25 | 2021-10-10 08:25:10 ==============
=> Current Lr: 3.125e-05
[0/375]: 0.0080
[20/375]: 0.0082
[40/375]: 0.0065
[60/375]: 0.0087
[80/375]: 0.0128
[100/375]: 0.0064
[120/375]: 0.0082
[140/375]: 0.0057
[160/375]: 0.0064
[180/375]: 0.0071
[200/375]: 0.0064
[220/375]: 0.0061
[240/375]: 0.0066
[260/375]: 0.0056
[280/375]: 0.0050
[300/375]: 0.0056
[320/375]: 0.0058
[340/375]: 0.0114
[360/375]: 0.0073
=> Training Loss: 0.0072, Evaluation Loss 0.0069

============= Epoch 26 | 2021-10-10 08:27:09 ==============
=> Current Lr: 3.125e-05
[0/375]: 0.0065
[20/375]: 0.0056
[40/375]: 0.0060
[60/375]: 0.0055
[80/375]: 0.0061
[100/375]: 0.0100
[120/375]: 0.0081
[140/375]: 0.0050
[160/375]: 0.0055
[180/375]: 0.0096
[200/375]: 0.0073
[220/375]: 0.0072
[240/375]: 0.0065
[260/375]: 0.0062
[280/375]: 0.0051
[300/375]: 0.0062
[320/375]: 0.0079
[340/375]: 0.0064
[360/375]: 0.0057
=> Training Loss: 0.0071, Evaluation Loss 0.0070

============= Epoch 27 | 2021-10-10 08:29:08 ==============
=> Current Lr: 3.125e-05
[0/375]: 0.0079
[20/375]: 0.0059
[40/375]: 0.0067
[60/375]: 0.0054
[80/375]: 0.0051
[100/375]: 0.0076
[120/375]: 0.0096
[140/375]: 0.0064
[160/375]: 0.0057
[180/375]: 0.0066
[200/375]: 0.0116
[220/375]: 0.0075
[240/375]: 0.0068
[260/375]: 0.0070
[280/375]: 0.0083
[300/375]: 0.0082
[320/375]: 0.0068
[340/375]: 0.0082
[360/375]: 0.0074
=> Training Loss: 0.0071, Evaluation Loss 0.0068

============= Epoch 28 | 2021-10-10 08:31:07 ==============
=> Current Lr: 3.125e-05
[0/375]: 0.0060
[20/375]: 0.0056
[40/375]: 0.0066
[60/375]: 0.0064
[80/375]: 0.0079
[100/375]: 0.0068
[120/375]: 0.0046
[140/375]: 0.0068
[160/375]: 0.0063
[180/375]: 0.0086
[200/375]: 0.0074
[220/375]: 0.0066
[240/375]: 0.0054
[260/375]: 0.0066
[280/375]: 0.0051
[300/375]: 0.0058
[320/375]: 0.0092
[340/375]: 0.0075
[360/375]: 0.0067
=> Training Loss: 0.0069, Evaluation Loss 0.0071

============= Epoch 29 | 2021-10-10 08:33:06 ==============
=> Current Lr: 3.125e-05
[0/375]: 0.0110
[20/375]: 0.0075
[40/375]: 0.0067
[60/375]: 0.0056
[80/375]: 0.0073
[100/375]: 0.0078
[120/375]: 0.0065
[140/375]: 0.0086
[160/375]: 0.0058
[180/375]: 0.0064
[200/375]: 0.0069
[220/375]: 0.0066
[240/375]: 0.0090
[260/375]: 0.0086
[280/375]: 0.0087
[300/375]: 0.0075
[320/375]: 0.0051
[340/375]: 0.0080
[360/375]: 0.0066
=> Training Loss: 0.0068, Evaluation Loss 0.0067

============= Epoch 30 | 2021-10-10 08:35:06 ==============
=> Current Lr: 1.5625e-05
[0/375]: 0.0056
[20/375]: 0.0056
[40/375]: 0.0054
[60/375]: 0.0070
[80/375]: 0.0063
[100/375]: 0.0049
[120/375]: 0.0059
[140/375]: 0.0128
[160/375]: 0.0042
[180/375]: 0.0067
[200/375]: 0.0057
[220/375]: 0.0130
[240/375]: 0.0059
[260/375]: 0.0052
[280/375]: 0.0078
[300/375]: 0.0083
[320/375]: 0.0073
[340/375]: 0.0075
[360/375]: 0.0050
=> Training Loss: 0.0068, Evaluation Loss 0.0066

============= Epoch 31 | 2021-10-10 08:37:09 ==============
=> Current Lr: 1.5625e-05
[0/375]: 0.0079
[20/375]: 0.0053
[40/375]: 0.0063
[60/375]: 0.0068
[80/375]: 0.0067
[100/375]: 0.0068
[120/375]: 0.0072
[140/375]: 0.0066
[160/375]: 0.0064
[180/375]: 0.0067
[200/375]: 0.0069
[220/375]: 0.0065
[240/375]: 0.0065
[260/375]: 0.0063
[280/375]: 0.0058
[300/375]: 0.0065
[320/375]: 0.0058
[340/375]: 0.0069
[360/375]: 0.0051
=> Training Loss: 0.0067, Evaluation Loss 0.0067

============= Epoch 32 | 2021-10-10 08:39:13 ==============
=> Current Lr: 1.5625e-05
[0/375]: 0.0054
[20/375]: 0.0059
[40/375]: 0.0073
[60/375]: 0.0057
[80/375]: 0.0093
[100/375]: 0.0058
[120/375]: 0.0080
[140/375]: 0.0073
[160/375]: 0.0083
[180/375]: 0.0061
[200/375]: 0.0074
[220/375]: 0.0057
[240/375]: 0.0061
[260/375]: 0.0045
[280/375]: 0.0055
[300/375]: 0.0055
[320/375]: 0.0053
[340/375]: 0.0045
[360/375]: 0.0080
=> Training Loss: 0.0066, Evaluation Loss 0.0065

============= Epoch 33 | 2021-10-10 08:41:14 ==============
=> Current Lr: 1.5625e-05
[0/375]: 0.0054
[20/375]: 0.0065
[40/375]: 0.0077
[60/375]: 0.0055
[80/375]: 0.0069
[100/375]: 0.0058
[120/375]: 0.0076
[140/375]: 0.0085
[160/375]: 0.0061
[180/375]: 0.0070
[200/375]: 0.0066
[220/375]: 0.0082
[240/375]: 0.0063
[260/375]: 0.0059
[280/375]: 0.0061
[300/375]: 0.0083
[320/375]: 0.0062
[340/375]: 0.0060
[360/375]: 0.0074
=> Training Loss: 0.0068, Evaluation Loss 0.0067

============= Epoch 34 | 2021-10-10 08:43:13 ==============
=> Current Lr: 1.5625e-05
[0/375]: 0.0048
[20/375]: 0.0066
[40/375]: 0.0059
[60/375]: 0.0071
[80/375]: 0.0076
[100/375]: 0.0071
[120/375]: 0.0053
[140/375]: 0.0046
[160/375]: 0.0085
[180/375]: 0.0081
[200/375]: 0.0063
[220/375]: 0.0045
[240/375]: 0.0071
[260/375]: 0.0054
[280/375]: 0.0081
[300/375]: 0.0046
[320/375]: 0.0063
[340/375]: 0.0049
[360/375]: 0.0055
=> Training Loss: 0.0066, Evaluation Loss 0.0066

============= Epoch 35 | 2021-10-10 08:45:11 ==============
=> Current Lr: 7.8125e-06
[0/375]: 0.0073
[20/375]: 0.0091
[40/375]: 0.0061
[60/375]: 0.0056
[80/375]: 0.0114
[100/375]: 0.0058
[120/375]: 0.0078
[140/375]: 0.0071
[160/375]: 0.0063
[180/375]: 0.0071
[200/375]: 0.0054
[220/375]: 0.0055
[240/375]: 0.0049
[260/375]: 0.0069
[280/375]: 0.0056
[300/375]: 0.0057
[320/375]: 0.0082
[340/375]: 0.0047
[360/375]: 0.0052
=> Training Loss: 0.0065, Evaluation Loss 0.0066

============= Epoch 36 | 2021-10-10 08:47:11 ==============
=> Current Lr: 7.8125e-06
[0/375]: 0.0032
[20/375]: 0.0067
[40/375]: 0.0062
[60/375]: 0.0063
[80/375]: 0.0072
[100/375]: 0.0063
[120/375]: 0.0077
[140/375]: 0.0065
[160/375]: 0.0072
[180/375]: 0.0078
[200/375]: 0.0061
[220/375]: 0.0055
[240/375]: 0.0062
[260/375]: 0.0077
[280/375]: 0.0061
[300/375]: 0.0083
[320/375]: 0.0067
[340/375]: 0.0096
[360/375]: 0.0063
=> Training Loss: 0.0065, Evaluation Loss 0.0065

============= Epoch 37 | 2021-10-10 08:49:09 ==============
=> Current Lr: 7.8125e-06
[0/375]: 0.0089
[20/375]: 0.0080
[40/375]: 0.0067
[60/375]: 0.0070
[80/375]: 0.0058
[100/375]: 0.0064
[120/375]: 0.0062
[140/375]: 0.0056
[160/375]: 0.0059
[180/375]: 0.0050
[200/375]: 0.0053
[220/375]: 0.0067
[240/375]: 0.0057
[260/375]: 0.0061
[280/375]: 0.0048
[300/375]: 0.0084
[320/375]: 0.0052
[340/375]: 0.0061
[360/375]: 0.0055
=> Training Loss: 0.0065, Evaluation Loss 0.0065

============= Epoch 38 | 2021-10-10 08:51:09 ==============
=> Current Lr: 7.8125e-06
[0/375]: 0.0061
[20/375]: 0.0055
[40/375]: 0.0058
[60/375]: 0.0095
[80/375]: 0.0048
[100/375]: 0.0059
[120/375]: 0.0111
[140/375]: 0.0050
[160/375]: 0.0062
[180/375]: 0.0066
[200/375]: 0.0064
[220/375]: 0.0067
[240/375]: 0.0069
[260/375]: 0.0042
[280/375]: 0.0039
[300/375]: 0.0055
[320/375]: 0.0061
[340/375]: 0.0094
[360/375]: 0.0066
=> Training Loss: 0.0065, Evaluation Loss 0.0065

============= Epoch 39 | 2021-10-10 08:53:09 ==============
=> Current Lr: 7.8125e-06
[0/375]: 0.0042
[20/375]: 0.0065
[40/375]: 0.0081
[60/375]: 0.0078
[80/375]: 0.0050
[100/375]: 0.0064
[120/375]: 0.0058
[140/375]: 0.0062
[160/375]: 0.0061
[180/375]: 0.0046
[200/375]: 0.0050
[220/375]: 0.0076
[240/375]: 0.0067
[260/375]: 0.0066
[280/375]: 0.0048
[300/375]: 0.0052
[320/375]: 0.0059
[340/375]: 0.0064
[360/375]: 0.0060
=> Training Loss: 0.0066, Evaluation Loss 0.0065
